# ðŸ§  LocalGPT - Agente LLM en tu Navegador

LocalGPT es un agente conversacional que funciona 100% en el navegador, usando modelos como LLaMA 3 con WebLLM. No requiere servidor, funciona offline tras la carga inicial, y permite chatear con un modelo de lenguaje directamente desde tu mÃ¡quina, de forma privada y personalizada. Usando tecnologÃ­as como **Web Workers**, **WebGPU** y [`@mlc-ai/web-llm`], ofrece una experiencia ideal para entornos educativos, demostraciones tÃ©cnicas o exploraciÃ³n local de modelos LLM.

---

## ðŸš€ InstalaciÃ³n y EjecuciÃ³n

### Requisitos

- Navegador moderno compatible con ES Modules, Web Workers y WebGPU (Chrome, Edge, Firefox).
- Modelo open source optimizado para WebLLM (Obtener desde https://github.com/mlc-ai/web-llm).
- Servidor HTTP local (no usar `file://` directamente).

### OpciÃ³n 1: Servidor simple con Node.js

```bash
npm install -g http-server
cd chatgpt-local
http-server
```

Accede desde: `http://127.0.0.1:8081`

### OpciÃ³n 2: Live Server en Visual Studio Code

1. Instala la extensiÃ³n **Live Server**.
2. Abre el proyecto en VSCode.
3. Haz clic derecho en `index.html` â†’ **Open with Live Server**.

---

## ðŸŽ¨ CaracterÃ­sticas

- Interfaz minimalista y adaptativa.
- Streaming de texto en tiempo real.
- Historial de conversaciÃ³n persistente (exportaciÃ³n/importaciÃ³n en JSON).
- BotÃ³n para copiar respuestas con un clic.
- SeparaciÃ³n visual entre entradas del usuario y respuestas del bot.
- Totalmente autÃ³nomo â€” *sin backend, APIs ni conexiÃ³n externa*.
- Compatible con mÃºltiples modelos open source optimizados.

---

## ðŸ”§ PersonalizaciÃ³n

- Cambia el modelo editando `MODELO_SELECCIONADO` en `engine.js`.
- Ajusta parÃ¡metros como `temperature`, `max_tokens` y mÃ¡s en `events.js` para determinar los parametros de respuesta.
- Edita el estilo visual en `style.css`.
- Personaliza el comportamiento del modelo (System Prompt).

Por ejemplo, puedes establecerlo como:

* `"Eres un programador experto en JavaScript."`
* `"ActÃºa como un tutor amigable que explica conceptos tÃ©cnicos a principiantes."`
* `"Eres un asistente profesional que responde de manera breve y precisa."`

AquÃ­ tienes el fragmento exacto donde puedes hacer la modificaciÃ³n (ubicado en el archivo 'events.js'):

```js
const mensajeSystem = {
  role: 'system',
  content: `Eres un programador experto en JavaScript que responde con ejemplos prÃ¡cticos.`
};
```

Este mensaje se envÃ­a al modelo antes que cualquier otro mensaje del usuario, y define su comportamiento durante toda la conversaciÃ³n.

---

## ðŸ—‚ï¸ Estructura del Proyecto

```
CHATGPT-LOCAL/
â”œâ”€â”€ .gitignore               # ExclusiÃ³n de archivos innecesarios
â”œâ”€â”€ README.md                # DocumentaciÃ³n del proyecto
â”œâ”€â”€ package.json             # Dependencias y scripts del proyecto
â”œâ”€â”€ package-lock.json        # ResoluciÃ³n exacta de dependencias
â””â”€â”€ src/
    â”œâ”€â”€ index.html           # Estructura principal del frontend
    â”œâ”€â”€ style.css            # Estilos visuales
    â”œâ”€â”€ main.js              # InicializaciÃ³n del programa
    â”œâ”€â”€ worker.js            # Web Worker para cargar el modelo
    â””â”€â”€ modules/
        â”œâ”€â”€ dom.js           # ManipulaciÃ³n del DOM
        â”œâ”€â”€ engine.js        # Motor de inferencia
        â”œâ”€â”€ events.js        # Manejo de eventos y logica
        â”œâ”€â”€ exportImport.js  # Persistencia de sesiones con JSON
        â””â”€â”€ messages.js      # LÃ³gica de conversaciÃ³n
```

---

## ðŸ§µ Â¿QuÃ© hace `worker.js`?

El archivo `worker.js` corre un **Web Worker** que:

- Carga el modelo de lenguaje optimizado en WebAssembly/WebGPU.
- Ejecuta inferencias de forma paralela al hilo principal.
- Mantiene fluidez en la interfaz mediante `postMessage`.

---

## âš ï¸ Advertencias

- El rendimiento depende de tu hardware (CPU, GPU, WebGPU disponible).
- Modelos pesados pueden no ser adecuados para dispositivos sin aceleraciÃ³n.
- Este proyecto estÃ¡ orientado netamente a exploraciÃ³n y aprendizaje, no a producciÃ³n.

---

## ðŸ“œ Licencia

Distribuido bajo la [licencia MIT](https://opensource.org/licenses/MIT). Apto para uso personal, educativo o colaborativo.

---

## ðŸ¤ Contribuciones

Â¡Siempre bienvenidas! Puedes colaborar con:

- Mejoras en estructura modular y rendimiento.
- Nuevas funcionalidades o integraciÃ³n de modelos.
- Correcciones en diseÃ±o, lÃ³gica o documentaciÃ³n.

---

## ðŸ“¦ Modelos Compatibles por Consumo de VRAM

### ðŸŸ¢ LIGEROS *(â‰¤ 1500 MB)*

| Modelo         | `model_id`                               | VRAM aprox. |
|----------------|-------------------------------------------|-------------|
| SmolLM-135M     | `"SmolLM2-135M-Instruct-q0f16-MLC"`       | 359 MB      |
| SmolLM-360M     | `"SmolLM2-360M-Instruct-q4f16_1-MLC"`     | 376 MB      |
| LLaMA-3 1B      | `"Llama-3.2-1B-Instruct-q4f16_1-MLC"`      | 879 MB      |
| Qwen 0.5B       | `"Qwen1.5-0.5B-Chat-q4f16_1-MLC"`          | 1372 MB     |
| Phi-3 Mini      | `"phi-3-mini-4k-instruct-q4f16_1-MLC"`     | 1464 MB     |

### ðŸŸ¡ MEDIANOS *(1501 â€“ 4000 MB)*

| Modelo                 | `model_id`                                    | VRAM aprox. |
|------------------------|-----------------------------------------------|-------------|
| SmolLM 1.7B            | `"SmolLM2-1.7B-Instruct-q4f16_1-MLC"`          | 1774 MB     |
| Qwen 1.5B              | `"Qwen1.5-1.5B-Chat-q4f16_1-MLC"`              | 2046 MB     |
| DeepSeek Coder 1.3B    | `"deepseek-coder-1.3b-instruct-q4f16_1-MLC"`   | 2111 MB     |
| LLaMA-3 3B (Hermes)    | `"Hermes-3-Llama-3.2-3B-q4f16_1-MLC"`          | 2263 MB     |
| Phi-3.5 Mini           | `"Phi-3.5-mini-instruct-q4f16_1-MLC"`          | 3672 MB     |

### ðŸ”´ PESADOS *(> 4000 MB)*

| Modelo                  | `model_id`                                    | VRAM aprox. |
|-------------------------|-----------------------------------------------|-------------|
| Mistral 7B Instruct     | `"Mistral-7B-Instruct-v0.3-q4f16_1-MLC"`       | 4573 MB     |
| LLaMA-3 8B (Hermes)     | `"Hermes-2-Theta-Llama-3-8B-q4f16_1-MLC"`      | 4976 MB     |
| Qwen 4B                 | `"Qwen1.5-4B-Chat-q4f16_1-MLC"`                | 4710 MB     |
| DeepSeek Coder 6.7B     | `"deepseek-coder-6.7b-instruct-q4f16_1-MLC"`   | 5222 MB     |
| Qwen2 7B                | `"Qwen2-7B-Instruct-q4f16_1-MLC"`              | 5645 MB     |

---
